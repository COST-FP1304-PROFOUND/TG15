---
title: "Experiments for TG15 with VSEM"
author: "David Cameron and Mike Dietze"
date: '`r Sys.Date()`'
output:
#  slidy_presentation: default
  html_document: default
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
#  beamer_presentation:
#    includes:
#      in_header: mystyles.sty
  word_document: default
---

```{r include=FALSE}
### SETTINGS
library(BayesianTools)
CLEAN.BUILD = TRUE
PLOT.DIAGNOSTICS = TRUE
defParms = c(1,3,5,6,9,10)
```

# Introduction

What is the effective information content (IC) of observations
 - One eddy covariance tower with 17000 measurements count this as n = 1 or n = 17000 obs?
 - Aggregate 5 min data to 10 min retain essentially same IC but sample size halved!
 - Spectral analysis of eddy covariance NEE data two peaks: annual(seasonal) and diurnal
 - Often assume in BC that each data-point provides independent information
 - If biases in data or model errors are not independent

 To-date solutions generally ad hoc and/or arbitrary
  - Ignore but then models over-fitted and uncertainty underestimated
  - Apply arbitrary weights to rebalance influence of data in BC 
  - Thin the number of eddy covariance obs 
   - throwing away useful information

 Artificial experiments
 - calibration data model's own output to control:
  1) model and data perfection/imperfection
  2) balance/unbalance of data in the calibration
 - developed a very simple ecosystem model (VSEM)  

# Methods

## Computational experiments

### Idealised experiment with virtual data from VSEM
 - Run model with a parameter vector output assign as 'truth'
 - Add noise to create idealised observations

```{r include=FALSE}
if(!file.exists("obs.RData")|CLEAN.BUILD){
  set.seed(123)
  ## ndays                 <- 366
  ndays                 <- 1000
  PAR                   <- VSEMcreatePAR(1:ndays)
  refPars               <- VSEMgetDefaults()
  ## merge non-identifiable parameters
  ## refPars[1,] <- refPars[1,]*refPars[2,]
  ## refPars[3,] <- refPars[3,]*(1-refPars[4,c(1,3,2)])
  ## refPars <- refPars[-c(2,4),]  
  nvar = nrow(refPars)+1
  ## add SD
  refPars[nvar,]          <- c(0.1, 0.001, 0.5)
  rownames(refPars)[nvar] <- "error-sd"
  ## calculate 'true' output and pseudodata
  referenceData         <- VSEM(refPars$best[1:(nvar-1)], PAR,vers=1) 
  obs                   <- referenceData + rnorm(length(referenceData), sd = (abs(referenceData) + 1E-7) * refPars$best[nvar])
  save(nvar,ndays,PAR,refPars,referenceData,obs,file="obs.RData")
  if(PLOT.DIAGNOSTICS){
    par(mfrow=c(3,1))
    for(i in 1:3){
      plot(obs[,i])
      lines(referenceData[,i],col=3,lwd=3)
    }
  }
} else {
  load("obs.RData")
}
```

### Identifying the issue

Variables:
 - Amount of data: high but balanced, low but balanced, unbalanced
 - Biases: perfect model, data bias, model structural error
 
Run a factorial experiment exploring all 9 combinations

Assess error and bias in model parameters, initial conditions, and  outputs. Because this is a simulated experiment we can compare to a known set of 'true' values.

### Assessing approaches 

Alternative likelihoods
- IID
- AR1
- additive bias

## VSEM

Photosynthesis

\begin{align}
NPP &= PAR \times LUE \times (1 - \exp^{(-KEXT \times C_v)})\\
\end{align}

 - PAR Photosynthetically active radiation
 - LUE Light use efficiency of NPP (Ra implicit) 
 - KEXT Beer's law light extinction coeff
 - $C_v$ Vegetation carbon

C-state equations
\begin{align}
\frac{dC_v}{dt}  &= A_v \times NPP &- \frac{C_v}{\tau_v} \\ 
\frac{dC_r}{dt}  &= (1.0-A_v) \times NPP &- \frac{C_r}{\tau_r}\\
\frac{dC_s}{dt}  &= \frac{C_r}{\tau_r} + \frac{C_v}{\tau_v} &- \frac{C_s}{\tau_s}
\end{align}
 - $C_v$, $C_r$ and $C_s$ : Carbon in vegetation, root and soil pools 

## Bayesian Calibration
 - Fit model to data using BayesianTools R package

```{r include=FALSE}
## HELPER FUNCTIONS

plotTimeSeriesResults <- function(x, model, observed, error = NULL, plotResiduals = T, start = 1,pool=1){
  
  if(inherits(x,"bayesianOutput")) parMatrix = getSample(x, start = start)
  else if (class(sampler) == "matrix") parMatrix = sampler
  else stop("wrong type give to variable sampler")
  
  myModel <- function(x) model(x,pool)
  pred <- getPredictiveIntervals(parMatrix = parMatrix, model = myModel, thin = 1000, quantiles = c(0.025, 0.5, 0.975), error = error)
  
  plotTimeSeries(observed = observed, predicted = pred[2,], confidenceBand = pred[c(1,3),])
  lines(pred[2,],col="red",lwd=2)
}

fitVSEM <- function(fname){
  if(!file.exists(fname)|CLEAN.BUILD){
    bayesianSetup <- createBayesianSetup(likelihood, prior,best = newPars[parSel], names = rownames(refPars)[parSel])
    settings = list(iterations = 20000)
    out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "DREAMzs", settings = settings)
    save(out,file=fname)
  } else {
    load(fname)
  }
  invisible(out)
}

runModel <- function(par,pool){
  x = createMixWithDefaults(par, newPars, parSel)
  predicted <- VSEM(x[1:(nvar-1)], PAR)
  return(predicted[,pool])
}

errorFunction <- function(mean, par) rnorm(length(mean), mean = mean, sd = abs(mean)*par[length(par)])

plotPosteriors <- function(out,refPars){
  np = length(parSel)  ## number of parameters fit
  outM <- as.matrix(out$chain)[,1:np]
  colnames(outM) <- rownames(refPars)[parSel]
  for(i in 1:np){
    true = refPars[parSel[i],1]
    d = density(outM[,i])
    xlim = range(c(d$x,true))
    plot(d,xlim=xlim,main=colnames(outM)[i])
    abline(v=true,col=2,lwd=3)
  }
}

vsemDiagnostics <- function(out,refPars){
  ## thin
  nmc = nrow(out$chain[[1]])
  out$chain <- window(out$chain,start=nmc/2,thin=(nmc/2)/5000)
  ## could replace above with getSample?
  
  par(mfrow = c(2,2))
  for(i in 1:3){
    myObs = obs[,i]
    if(exists("obsSel") & i %in% isLow) myObs[-obsSel] <- NA
    plotTimeSeriesResults(x = out, model = runModel, observed = myObs, error = errorFunction,pool=i)
    lines(referenceData[,i],col=3,lwd=1)
  }
  
  if(PLOT.DIAGNOSTICS){
    
    #plot(out$chain)
    
    par(mfrow=c(3,2))
    plotPosteriors(out,refPars)
    
    par(mfrow=c(1,1))
    correlationPlot(out)
  }
}

vsemScore <- function(out){
  parMatrix = getSample(out)
  skill = matrix(NA,3,2)
  for(pool in 1:3){
    myModel <- function(x) runModel(x,pool)
    pred = getPredictiveDistribution(parMatrix, model = myModel, thin = 100)
    bias = mean(apply(pred,1,function(y){mean(referenceData[,pool]-y)}))
    rmse = mean(apply(pred,1,function(y){sqrt(mean((referenceData[,pool]-y)^2))}))
    skill[pool,] <- c(rmse,bias)/mean(referenceData[,pool])
  }
  row.names(skill) <- c("NEE","Cv","Cs")
  colnames(skill) <- c("rmse","bias")
  return(skill)
}

```

# Results 

## Identifying model fit problems

```{r include=FALSE}
skill <- list()
```

### Perfect model balanced perfect data 
Gaussian 1000 obs for each of NEE, Cv and Cs

```{r include=FALSE}
newPars <- refPars$best
parSel = c(defParms, nvar)
rm(obsSel)
isLow = NULL
likelihood <- function(x, sum = TRUE){
  x         <- createMixWithDefaults(x, newPars, parSel)
  predicted <- VSEM(x[-nvar], PAR,vers=1)
  diff      <- c(predicted[,1:3] - obs[,1:3])
  llValues  <- dnorm(diff, sd = (abs(c(predicted[,1:3])) + 0.0000001) * x[nvar], log = T) 
  if (sum == FALSE) return(llValues)
  else return(sum(llValues))
}

prior         <- createUniformPrior(lower = refPars$lower[parSel], upper = refPars$upper[parSel])
out1 <- fitVSEM("run1.RData")
```

```{r echo=FALSE, background='white', fig.height=7, fig.cap="Perfect model, perfect data"}
vsemDiagnostics(out1,refPars)
skill[[1]] <-vsemScore(out1) 
knitr::kable(skill[[1]])
```

### Perfect model unbalanced perfect data 
Gaussian 1000 obs for NEE and Cs and only 6 for Cv

```{r include=FALSE}
newPars <- refPars$best
parSel = c(defParms, nvar)
obsSel <- c(1,202,390,550,750,920)
isLow = 2 ## which variables has few observations
likelihood <- function(x, sum = TRUE){
  x         <- createMixWithDefaults(x, newPars, parSel)
  predicted <- VSEM(x[1:(nvar-1)], PAR)
  diff       <- c(predicted[,c(1,3)] - obs[,c(1,3)])
  llValues1  <- dnorm(diff, sd = (abs(c(predicted[,c(1,3)])) + 0.0000001) * x[nvar], log = T) 

  diff       <- c(predicted[obsSel,2] - obs[obsSel,2])
  llValues2  <- dnorm(diff, sd = (abs(c(predicted[obsSel,2])) + 0.0000001) * x[nvar], log = T) 

  if (sum == FALSE) return(llValues)
  else return(sum(llValues1,llValues2))
}

## Prior
prior         <- createUniformPrior(lower = refPars$lower[parSel], upper = refPars$upper[parSel])

out2 <- fitVSEM("run2.RData")
```

```{r echo=FALSE, background='white', fig.height=7, fig.cap="Perfect model, unbalanced data"}
vsemDiagnostics(out2,refPars)
skill[[2]] <-vsemScore(out2) 
knitr::kable(skill[[2]])
```

### Perfect model, low-volume perfect data 
Gaussian, only 6 observations for NEE. Cs, and Cv

```{r include=FALSE}
newPars <- refPars$best
parSel = c(defParms, nvar)
obsSel <- c(1,202,390,550,750,920)
isLow = 1:3 ## which variables has few observations
likelihood <- function(x, sum = TRUE){
  x         <- createMixWithDefaults(x, newPars, parSel)
  predicted <- VSEM(x[1:(nvar-1)], PAR)
  diff       <- c(predicted[obsSel,] - obs[obsSel,])
  llValues  <- dnorm(diff, sd = (abs(c(predicted[obsSel,])) + 0.0000001) * x[nvar], log = T) 

  if (sum == FALSE) return(llValues)
  else return(sum(llValues))
}

## Prior
prior         <- createUniformPrior(lower = refPars$lower[parSel], upper = refPars$upper[parSel])

out9 <- fitVSEM("run9.RData")
```

```{r echo=FALSE, background='white', fig.height=7, fig.cap="Perfect model, low volume data"}
vsemDiagnostics(out9,refPars)
skill[[3]] <-vsemScore(out9) 
knitr::kable(skill[[3]])
```

### Model with error balanced perfect data (1000 obs) 
Remove the root pool by setting allocation to vegetation Av = 1.0

```{r include=FALSE}

## Likelihood: Gaussian 1000 obs for each of NEE, Cv and Cs
newPars <- refPars$best
names(newPars) = row.names(refPars)
newPars["Av"]  <- 1.0
newPars["Cr"] <- 0.0
parSel = c(defParms, nvar)
## parSel = c(defParms[-which(names(newPars)%in% c("Av","Cr","tauR"))], nvar)
rm(obsSel)
isLow = NULL
likelihood <- function(x, sum = TRUE){
  x         <- createMixWithDefaults(x, newPars, parSel)
  predicted <- VSEM(x[1:11], PAR)
  diff      <- c(predicted[,1:3] - obs[,1:3])
  llValues  <- dnorm(diff, sd = (abs(c(predicted[,1:3])) + 0.0000001) * x[nvar], log = T) 
  if (sum == FALSE) return(llValues)
  else return(sum(llValues))
}

prior         <- createUniformPrior(lower = refPars$lower[parSel], upper = refPars$upper[parSel])

out3 <- fitVSEM("run3.RData")
```

```{r echo=FALSE, background='white', fig.height=7, fig.cap="Output from error model after calibration"}
vsemDiagnostics(out3,refPars)
skill[[4]] <-vsemScore(out3) 
knitr::kable(skill[[4]])
```

### Model with error balanced perfect data (6 obs) 
Remove the root pool by setting allocation to vegetation Av = 1.0
Gaussian 6 obs for each of NEE, Cv and Cs

```{r include=FALSE}
newPars <- refPars$best
names(newPars) = row.names(refPars)
newPars["Av"]  <- 1.0
newPars["Cr"] <- 0.0
parSel = c(defParms, nvar)
## parSel = c(defParms[-which(names(newPars)%in% c("Av","Cr","tauR"))], nvar)
obsSel <- c(1,202,390,550,750,920)
isLow = 1:3
## obsSel = seq(1,366,72)
likelihood <- function(x, sum = TRUE){
  x         <- createMixWithDefaults(x, newPars, parSel)
  predicted <- VSEM(x[1:11], PAR)
  diff      <- c(predicted[obsSel,1:3] - obs[obsSel,1:3])
  llValues  <- dnorm(diff, sd = (abs(c(predicted[obsSel,1:3])) + 0.0000001) * x[nvar], log = T) 
  if (sum == FALSE) return(llValues)
  else return(sum(llValues))
}

prior         <- createUniformPrior(lower = refPars$lower[parSel], upper = refPars$upper[parSel])
out4 <- fitVSEM("run4.RData")
```

```{r echo=FALSE, background='white', fig.height=7, fig.cap="Output from perfect model after calibration"}
vsemDiagnostics(out4,refPars)
skill[[5]] <-vsemScore(out4) 
knitr::kable(skill[[5]])
```

### Model with error unbalanced perfect data 
Remove the root pool by setting allocation to vegetation Av = 1.0
Gaussian 1000 obs for NEE and Cs and only 6 for Cv

```{r include=FALSE}
newPars <- refPars$best
names(newPars) = row.names(refPars)
newPars["Av"]  <- 1.0
newPars["Cr"] <- 0.0
parSel = c(defParms, nvar)
## parSel = c(defParms[-which(names(newPars)%in% c("Av","Cr","tauR"))], nvar)
obsSel <- c(1,202,390,550,750,920)
isLow = 2
likelihood <- function(x, sum = TRUE){
  x         <- createMixWithDefaults(x, newPars, parSel)
  predicted <- VSEM(x[1:11], PAR)
  diff       <- c(predicted[,c(1,3)] - obs[,c(1,3)])
  llValues1  <- dnorm(diff, sd = (abs(c(predicted[,c(1,3)])) + 0.0000001) * x[nvar], log = T) 

  diff       <- c(predicted[obsSel,2] - obs[obsSel,2])
  llValues2  <- dnorm(diff, sd = (abs(c(predicted[obsSel,2])) + 0.0000001) * x[nvar], log = T) 

  if (sum == FALSE) return(llValues)
  else return(sum(llValues1,llValues2))
}

prior         <- createUniformPrior(lower = refPars$lower[parSel], upper = refPars$upper[parSel])

out5 <- fitVSEM("run5.RData")
```

```{r echo=FALSE, background='white', fig.height=7, fig.cap="Model error, unbalanced data"}
vsemDiagnostics(out5,refPars)
skill[[6]] <-vsemScore(out5) 
knitr::kable(skill[[6]])
```

### Perfect model balanced data that has an additive bias 
Cs obs have an additive error of 4

```{r include=FALSE}
obs.orig <- obs
obs[,3] <- obs[,3] + 4.0

newPars <- refPars$best
parSel = c(defParms, nvar)
rm(obsSel)
isLow = NULL
likelihood <- function(x, sum = TRUE){
  x         <- createMixWithDefaults(x, newPars, parSel)
  predicted <- VSEM(x[1:11], PAR)
  diff      <- c(predicted[,1:3] - obs[,1:3])
  llValues  <- dnorm(diff, sd = (abs(c(predicted[,1:3])) + 0.0000001) * x[nvar], log = T) 
  if (sum == FALSE) return(llValues)
  else return(sum(llValues))
}

prior         <- createUniformPrior(lower = refPars$lower[parSel], upper = refPars$upper[parSel])

out6 <- fitVSEM("run6.RData")
```

```{r echo=FALSE, background='white', fig.height=7, fig.cap="Perfect model, biased data"}
vsemDiagnostics(out6,refPars)
obs <- obs.orig
skill[[7]] <-vsemScore(out6) 
knitr::kable(skill[[7]])
```

### Perfect model unbalanced data that has an additive bias
Cs obs have an additive error of 4

```{r include=FALSE}
obs <- obs.orig
obs[,3] <- obs[,3] + 4.0

newPars <- refPars$best
parSel = c(defParms, nvar)
obsSel <- c(1,202,390,550,750,920)
isLow = 2
likelihood <- function(x, sum = TRUE){
  x         <- createMixWithDefaults(x, newPars, parSel)
  predicted <- VSEM(x[1:(nvar-1)], PAR)
  diff       <- c(predicted[,c(1,3)] - obs[,c(1,3)])
  llValues1  <- dnorm(diff, sd = (abs(c(predicted[,c(1,3)])) + 0.0000001) * x[nvar], log = T) 

  diff       <- c(predicted[obsSel,2] - obs[obsSel,2])
  llValues2  <- dnorm(diff, sd = (abs(c(predicted[obsSel,2])) + 0.0000001) * x[nvar], log = T) 

  if (sum == FALSE) return(llValues)
  else return(sum(llValues1,llValues2))
}

prior         <- createUniformPrior(lower = refPars$lower[parSel], upper = refPars$upper[parSel])

out7 <- fitVSEM("run7.RData")
```

```{r echo=FALSE, background='white', fig.height=7, fig.cap="Biased, unbalanced data"}
vsemDiagnostics(out7,refPars)
skill[[8]] <-vsemScore(out7) 
knitr::kable(skill[[8]])
obs <- obs.orig
```

### Perfect model low-volume data that has an additive bias
Cs obs have an additive error of 4

```{r include=FALSE}
obs <- obs.orig
obs[,3] <- obs[,3] + 4.0

newPars <- refPars$best
parSel = c(defParms, nvar)
obsSel <- c(1,202,390,550,750,920)
isLow = 1:3
likelihood <- function(x, sum = TRUE){
  x         <- createMixWithDefaults(x, newPars, parSel)
  predicted <- VSEM(x[1:(nvar-1)], PAR)

  diff       <- c(predicted[obsSel,] - obs[obsSel,])
  llValues  <- dnorm(diff, sd = (abs(c(predicted[obsSel,])) + 0.0000001) * x[nvar], log = T) 

  if (sum == FALSE) return(llValues)
  else return(sum(llValues))
}

prior         <- createUniformPrior(lower = refPars$lower[parSel], upper = refPars$upper[parSel])

out10 <- fitVSEM("run10.RData")
```

```{r echo=FALSE, background='white', fig.height=7, fig.cap="Biased low-volume data"}
vsemDiagnostics(out10,refPars)
skill[[9]] <-vsemScore(out10) 
knitr::kable(skill[[9]])
obs <- obs.orig
```

### Summary table

## note: in root 

| Experiment | Model | Data bias | Data volume | Model RMSE | Model Bias |
|:-----------|------:|----------:|------------:|-----------:|-----------:|


## Addressing errors

### Perfect model unbalanced data that has an additive bias with systmatic bias parameter
Cs obs have an additive error of 4
Gaussian 1000 obs for NEE and Cs and only 6 for Cv

```{r include=FALSE}
obs <- obs.orig
obs[,3] <- obs[,3] + 4.0

addPars <- refPars
addPars[nvar+1,] <- c(4.0, 0.0, 12.0)
row.names(addPars)<-c(row.names(refPars),"EsysNEE")
newPars <- addPars$best
parSel = c(defParms, nvar,nvar+1)
obsSel <- c(1,202,390,550,750,920)
isLow = 2
likelihood <- function(x, sum = TRUE){
  x         <- createMixWithDefaults(x, newPars, parSel)
  predicted <- VSEM(x[1:(nvar-1)], PAR)
#  modobs[,3]<-modobs[,3]+(x[nvar+1]-6.0)
  predicted[,3] <- predicted[,3] + x[nvar+1]
  
  diff       <- c(predicted[,c(1,3)] - obs[,c(1,3)])
  llValues1  <- dnorm(diff, sd = (abs(c(predicted[,c(1,3)])) + 0.0000001) * x[nvar], log = T) 

  diff       <- c(predicted[obsSel,2] - obs[obsSel,2])
  llValues2  <- dnorm(diff, sd = (abs(c(predicted[obsSel,2])) + 0.0000001) * x[nvar], log = T) 

  if (sum == FALSE) return(llValues)
  else return(sum(llValues1,llValues2))
}


prior         <- createUniformPrior(lower = addPars$lower[parSel], upper = addPars$upper[parSel])

out8 <- fitVSEM("run8.RData")
```

```{r echo=FALSE, background='white', fig.height=7, fig.cap="Biased, unbalanced data; Additive bias likelihood"}
vsemDiagnostics(out8,addPars)
obs <- obs.orig
```



# Short term ideas
## Experiment that includes obs bias(es)
 - Additive and or multiplicative for now 
 - Add multiplicative/additive bias terms to model and obs

## Test other model errors?
 - for example: include/remove Q10 type soil respiration 

# Longer term ideas
## use the formalism of Rougier(2007) in BC
The idea here is to learn about the inclusion of errors by analysing model data mismatch. Correlations between the mismatches of NEE, Cv and Cs could then be included in the calibration 
via Gaussian $\mu$ and $\sigma$ terms using the formalism of Rougier(2007). 

## include GAMMS in BC

## calibrate a simplier model to the output of a more complex model
Before a calibration against real obs an experiment where we create virtual data form a complex data 
and try to fit a simple model using what we have learned already about how to include terms that represent 
model structural error in the BC. 




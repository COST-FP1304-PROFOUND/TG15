# Discussion

## Identifying the issue with unbalanced dataset BC 

 * Unbalanced data are not an issue though uncertainty is larger.
 * For a model with a significant structural error or systematic bias
   in the calibration data parameters 'absorb' the error so that model
   output is not too far away from the data.
 * With a model structural error or data with a systematic bias
   general sense is that sparse data are somewhat ignored in the BC in
   favour of the plentiful data.
   * This is what we often observe with unbalanced datasets in BC. For
     example, Cameron et al (2018) but results here make it apparent
     that the issue is the presence of the structural error in the
     model rather than that the calibration data are unbalanced.
	 
## Diagnostic tool introduced

 As the Box aphorism says "All models are wrong but some are useful". A key consideration then is to what extent errors in the model (and biases in the data) give rise to the issues in calibration with imbalanced data that we have identified here. Or to put it another way, can we identify a signature for the issue that can be diagnosed so that modellers will know whether and how sever the issue may be in their calibration? To help with this we have developed a methodology that can be used as a diagnostic tool. We perform the diagnosis by starting with equal numbers of observations in the calibration and incrementally increasing the imbalance by allowing in more of the plentiful observations. If after calibration, model RMS errors versus observations increase for the sparse observations whilst RMS errors decrease for the plentiful observations, as shown in Fig. (\ref{fig:errModunbalDataBiasmodLikeOut}), as the imbalance increases, then we know that our calibration has this issue. If we create the diagnostic graph then it is possible also to assess how sever the problem is and at what point the increasing imbalance in the data starts to have a significant impact on the calibration. To overcome the issue there are ad-hoc measures that can be taken but as we have identified here, the underlying issue that needs to be addressed to make progress is to take account of model structural and data bias errors in the calibration. 
 
## Representing model and data error in BC helps to alleviate the issue

 * In this very simple example we were able to demonstrate a
   significant improvement by including terms in the likelihood to
   represent model and data error. 
   * In more real-world applications, representing model and data error
     in BC will be much more challenging but the analysis demonstrated
     here shows how to deal with the issue of calibrating with
     unbalanced datasets without resorting to ad hoc methods.
	 
## Is observational error too simple?
 * Likelihood term is the same as the observational error term.

## Eddy covariance data doesn’t close the budget
 * Would never be able to match the data with a model that conserves energy.

## Model with error and balanced data doesn’t show that the model has an error. 
 * Model is able to match the data quite well so doesn't 'uncover' the serious model structural error. 	 
 
# References

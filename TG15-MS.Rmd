---
title: "Identifying why Bayesian Calibration of process-based models with unbalanced quantities of calibration data can be challenging: The significance of model structural deficiencies and data biases."
author: "D. R. Cameron, F. Hartig, F. Minnuno, J. Oberpriller, B. Reineking, M. Van Oijen and M. Dietze"
date: '`r Sys.Date()`'
output:
#  slidy_presentation: default
#   html_document: default
#   md_document:
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: no
    keep_tex: true	
abstract: |
  Calibrating ecological process-based models using multiple data streams often improves the identifiability of model parameters, provides model predictions after calibration that are more consistent with underlying processes and helps to avoid compensating errors. However, using multiple constraints can sometimes also lead to problems, for example causing predictions for some variables to get worse, and this is particularly common when combining data sources with very different sample sizes. Such unbalanced model-data fusion efforts are becoming increasingly common, for example when combining manual and automated measurements. Here we use a series of simulated data experiments to illustrate the challenges that can occur when combining data, with a particular focus on unbalanced data and the role of systematic errors (i.e. biases) in models and data. We find that unbalanced data by itself is not the problem -- when fitting simulated data to the “true” model we can correctly recover model parameters and the true dynamics of latent variables even in the presence of (random) observation errors. Despite the popularity of approaches that focus on “weighting” data sets differently, this illustrates that the issue at hand is not one of sample size or information content per se. However, when there are systematic errors in the model or the data, we cannot recover the correct parameters, and the modeled dynamics of the low data volume variables can often depart significantly from the true values. To address these challenges we provide a diagnostic tool to help identify the impacts of unbalanced data and explore ways to account for systematic errors within statistical Likelihood functions. We show that even a simple linear bias correction can greatly improve the ability of the model to fit low-volume data and make accurate predictions. While doing so does not necessarily allow the model to recover the “true” model parameters, it does help us identify the presence of structural errors.
#  beamer_presentation:
#    incremental: true
#    includes:
#      in_header: mystyles.sty
#  word_document: default
#bibliography: bibliography.bibtex
---
 
```{r include=FALSE}
### SETTINGS
library(BayesianTools)
library(tidyverse)
library(gridExtra)
CLEAN.BUILD = FALSE

## PLOT.DIAGNOSTICS = TRUE
defParms = c(1,3,5,6,9,10) 
exptPath = "RDataWorkingMSILongNew/"
#exptPath = "~/OneDrive - NERC/COST/TG15/RDataWorkingMSILong/"
```

# Introduction

Modellers in ecology and earth system sciences increasingly rely on complex computer simulations (Fisher and Koven 2020, Fisher et al 2018), coupled with methods to combine models and data to generate precise forecasts and improve system understanding. 

In principle, the process of constraining model uncertainties via calibration (aka inverse modelling or model-data fusion, e.g. Hartig et al., 2012, Dietze et al. 2018) is relatively straightforward. The idea is to infer models (and their parameters) that are in agreement with the observed ecological and environmental data. This can be achieved via informal calibration or optimization procedures (citations), but as increasingly more data has become available in the recent years (citations), the field has moved towards formal statistical calibration methods based on likelihood or Bayesian statistics (CITATIONS). The methods allow formal parameter estimations for complex system models, and to project these uncertainties to model predictions, for example under climate change (citation). 
 
In practice however, the Bayesian Calibration (BC) of ecological process-based models can be challenging. For example there can be difficulties in calibrating process-based models with observational data representing multiple parts of the system being predicted by the model. In principle, the combination of heterogeneous data types in BC is straightforward, and simply amounts to multiplying the likelihoods for the individual data streams. In practice, however, researchers often observe problems when attempting a naive calibration with multiple constraints. Issues are particularly common when the data streams differ greatly in the quantity of data available. Such an imbalanced calibration dataset is now common since low-volumes of manually-collected field data are frequently combined with high-volumes of automatically-collected data either from in situ sensors (e.g. eddy-covariance) or via remote sensing. A common observation in this situation is that the outputs from BC (e.g. calibrated model parameters) are virtually identical to the results achieved by fitting the model to the high-volume data by itself, and in obvious disagreement with the lower-volume data type (e.g. Cameron et al in review 2018). 
 
Since each data point is usually modelled as an independent piece of information in a Likelihood, the influence of the sparse observations can often be overwhelmed by the higher frequency data (Cameron et al in review 2018). In essence, the data fusion ignores the low volume data, which gets swamped by the much larger sample size of the high volume data. Moreover, if the model cannot achieve a good fit to both data streams at the same time, it will logically favour fitting the common data stream, at the expense of worse predictions for model outputs with fewer data (Oberpriller et al., 2021).

This is highly undesirable as the lower-volume data often represents a part of the system that is crucial for the future projections (eg soil carbon and nitrogen), has relatively high uncertainty and has required higher labour costs to collect. As increasingly more data becomes available this issue of extremely imbalanced datasets is likely to worsen significantly. For example, NASA’s earth observation system is expected to grow by an order of magnitude, from an already overwhelming ~5PB/yr in 2018-2020 to a staggering ~50PB/yr, as soon as 2022 (https://earthdata.nasa.gov/eosdis/cloud-evolution).

Since the apparent issue is the large imbalance in the data it would then seem naively logical to try and correct that balance in some way. Unfortunately it is difficult to follow such an approach without resorting to ad-hoc solutions. For example, it is common to thin-out, aggregate or reweigh the calibration datasets so that they have a more balanced influence on the BC. The main purpose is to down-weigh the high-volume data so that the different data models are more balanced. For example, Medvigy et al 2009 constrained the ED2 model to nine data constraints, including eddy covariance at the annual, monthly, and hourly scale and forest growth and mortality data, and weighted each part of the likelihood equally. (Keenan et al. 2013) similarly weighted each dataset equally when calibrating the FöBAAR model to 16 distinct data constraints. [Cailleret et al. 2020] also equally weighted basal area increment and stem number distribution in the calibration of the forest model ForClim. (Thum et al. 2017) constrained ORCIDEE with multiple constraints, weighting each by sample size. (Richardson et al. 2010) calibrated the DALEC model by optimizing the product of the log-Likelihoods across six data constraints, which similarly weights all data sets as equally important. Unfortunately, this approach has no basis in probability theory largely since it makes no logical sense that the weight or significance of a dataset in the calibration should be determined by the presence of another more sparse dataset. The significance of a dataset in the calibration should be determined the reliability of that dataset alone. By arbitrarily changing the reliability of the calibration data we are also throwing away potentially very useful information that can be used to improve out models. Indeed the purpose of BC is to update our subjective prior knowledge with more objective evidence taken from observations. By reweighing the data we reintroduce our subjective control over the calibration by some measure of how close we want the model to fit the observations after calibration. While pragmatic, reweighing the observation does not seem to be the right approach unless there are no other possible solutions. Much better then is to form our solutions based on the underlying reasons for why unbalanced data cause challenges in the BC of ecological PBMs. 

The underlying reasons for challenges with unbalanced data in the BC of process-based models, namely model and data error, are known (Oberpriller et al., 2021) but are not immediately apparent and needs to be more widely understood to help avoid the practise of resorting to ad-hoc and hence fundamentally less useful approaches. The aim of this paper is firstly to identify and hence illustrate as clearly as possible using clear-cut virtual data experiments the underlying reasons for the issues that are commonly found when using unbalanced datasets in BC. Secondly we offer a diagnostic tool to help researchers identify whether issues that they are facing in BC could be attributed to the interaction of imbalanced calibration data with model/data error rather than some other cause. Finally we illustrate, as simply as possible, an approach to BC that avoids the need to make ad-hoc choices. 

# Methods

## VSEM model

Here we present the Very Simple Ecosystem Model (VSEM). The model was created to help illustrate the
main ideas that we present here. The model was designed to be very simple rather then realistic, but
yet resemble many typical, but more complicated, process-based ecosystem models (PBMs) that are
commonly used in carbon growth type ecosystem modelling.

In essence, the model determines the accumulation of carbon in the plant and soil from the growth of
the plant via photosynthesis and senescence to the soil which respires carbon back to the
atmosphere. The timestep of the VSEM is daily. 

### VSEM input data: Photosythetically active radiation (PAR)

The VSEM requires only one input dataset to drive the model namely daily PAR. 

Since we are interested in virtual experiments here we generate the PAR input data using an
sinusoidal function.

\begin{align}
PAR &= (\lvert \sin ( Days /365 \times \pi)  + \epsilon \rvert) \times 10 \\
\end{align}

 * PAR Photosynthetically active radiation
 * $\epsilon$ Gaussian noise added
 * Days number of days

### Photosynthesis equation

The model calculates Gross Primary Productivity (GPP) using a very simple light-use efficiency (LUE)
formulation multiplied by light interception. Light interception is calculated via Beer's law with a
constant light extinction coefficient operating on Leaf Area Index (LAI).  A parameter (GAMMA)
determines the fraction of GPP that is autotrophic respiration, giving the Net Primary Productivity (NPP).

\begin{align}
GPP &= PAR \times LUE \times (1 - \exp^{(-KEXT \times LAR \times C_v)})\\
NPP &= (1 - GAMMA) * GPP
\end{align}

 * PAR Photosynthetically active radiation (MJ $m^{-2}$ $day^{-1}$)
 * LUE Light use efficiency of NPP (Ra implicit)
 * KEXT Beer's law light extinction coeff
 * $C_v$ Vegetation carbon
 * LAR is the leaf area ratio
 * GAMMA is the ratio of autotrophic respiration to GPP

### Carbon pool state equations

There are three state equations [@Gill] representing the change in time of vegetation ($C_v$), root ($C_r$)
and soil ($C_s$) carbon pools. The Net Primary Productivity (NPP) is allocated to above (vegetation)
and below(root) ground carbon pools via a fixed allocation fraction. Carbon is lost from the plant
pools to a single soil pool via fixed vegetation and root turnover rates. Heterotropic respiration
in the soil is determined via a soil turnover rate.

\begin{align}
\frac{dC_v}{dt}  &= A_v \times NPP &- \frac{C_v}{\tau_v} \\
\frac{dC_r}{dt}  &= (1.0-A_v) \times NPP &- \frac{C_r}{\tau_r}\\
\frac{dC_s}{dt}  &= \frac{C_r}{\tau_r} + \frac{C_v}{\tau_v} &- \frac{C_s}{\tau_s}
\end{align}

### VSEM model parameters

| parameter name                     | variable name |
|:-----------------------------------|---------------|
| Light extinction coeff             | KEXT          |
| Leaf area ratio                    | LAR           |
| Light use efficiency               | LUE           |
| Ratio of autotrophic resp to GPP   | GAMMA         |
| Vegetation turnover rate           | tauV          |
| Soil decomposition rate            | tauS          |
| Root turnover rate                 | tauR          |
| Allocation frac to vegetation      | Av            |
| Initial vegetation pool size       | Cv            |
| Initial soil pool size             | Cs            |
| Initial root pool size             | Cr            |

## Bayesian Calibration

In Bayesian Calibration, our aim is to quantify the probability of the model parameters ($\theta$) being correct given the calibration data (P($\theta$ | D). Since this is not straightforward to calculate we make use of Bayes equation. 

\begin{equation}
P(\theta | D) \propto P(\theta) L(D | \theta)
\end{equation}

Where P($\theta$ | D), P($\theta$) and L(D|$\theta$) are known as the posterior, prior and likelihood respectively. Since it is not possible to calculate the likelihood for a numerical model such as VSEM analytically we sample from it and the prior using a Monte Carlo approach to sample from the posterior. As a way of making this sampling more efficient we use the DREAMzs algorithm in a Markov Chain Monte Carlo (MCMC) sampling  

 * brief summary of DREAMzs algorithm

The DREAMzs algorithm and MCMC functions that we use here are from the BayesianTools package. 

### Prior

Here we adopt a very simple uniform prior since our aim here is to identify the issue using a simple and therefore easy to interpret modelling approach. 

We need to be able to set the values for two parameters, allocation to vegetation (Av) and initial root pool for the virtual experiments described below. Since the root pool is not part of the model with the error we also exclude tauR from the calibration 

Of the remaining parameters LAR and GAMMA were removed from the calibration to avoid nonidentifiability issues. 

The remaining parameters are listed below along with the uniform prior ranges used. 

| parameter | min    |  max   |
|:----------|--------|-------|
| KEXT      | 0.2    | 1.0   |
| LUE       | 0.0002 | 0.004 |
| tauV      | 200    | 3000  |s
| tauS      | 4000   | 50000 |
| Cv        | 0.0    | 400   | 
| Cs        | 0.0    | 1000  |

## Idealised experiments with virtual data from VSEM
```{r child = 'TG15-virtualData.Rmd'}
```

### Likelihood 

Given that we added Gaussian noise to the model output to produce the virtual data, a univariate Gaussian likelihood is the obvious choice. In section (\@ref(modLike)) we discuss modifications to this simple likelihood to represent model structural error and data systematic bias. 

### Perfect model

A central theme that we consider here is the significance of a perfect model structure with all the
processes modelled perfectly. The only way to ensure a perfect model is to take the output from the
VSEM and consider this as virtual data in the BC. Gaussian noise is added to the model output to
represent system varability that is not captured by the model (as is away the case) but crucially
can be represented perfectly by the likelihood function that we use in the BC. The observations are
for the full 2048 day length of the VSEM for NEE, vegetative carbon and soil carbon.

For the vegetaive carbon we create a sparse dataset to simulate having an imbalance between
observations available for vegetative carbon, soil carbon and NEE. The sparse dataset has six
observations for days 2, 404, 780, 1100, 1500 and 1840.

### Model with known structural error

To simulate a model with a known structural error we consider a situation where a major model
process/structure is unknown and therefore missing in the model. Here we remove the root pool
completely from the VSEM to simulate a major structural error. This is done by initialising the root pool to zero and setting the root allocation fraction to zero so that all the NPP is now allocated to the vegetation pool. This also of course shuts off any senescence from the root pool to the soil. This gives the model a major structural error as we might have in a real situation whilst being sufficiently simple that we can still interpret the influence of the error.

### Observational data with known bias

In addition to considering model structural error, we also wish to investigate the influence of observations with biases since all observational data will to a greater or lesser extent contain biases. Here we simulate data biases by multiplying the soil data by 0.8 to represent a considerable multiplicative bias in the observations of soil carbon. 

## Modified Likelihood to represent structural errors in the model and systematic biases in the data. {modLike}

A general principle in modelling is to begin with the simplest approach and only move on to more complicated solutions if the simple approach fails. We adopt that approach here, by representing model structural error and data systematic bias in the likelihood function by very simple multiplicative and additive constants to the model outputs. We added terms for each of the three outputs for which we have calibration data namely (NEE, Cs and CV). Therefore we have six extra parameters to represent addition and multiplicative error for each of NEE, soil carbon and aboveground carbon (modaddNEE, modmultNEE, modaddCs, modmultCs,modaddCv and modmultCv). The priors for each of these are as follows. 

| parameter name |  min  | max  |
|:---------------|-------|------|
| modmultNEE     |  0.1  | 2.0  |
| modmultCs      |  0.1  | 2.0  |
| modmultCv      |  0.1  | 2.0  |
| modaddNEE      | -0.01 | 0.01 |
| modaddCs       | -1.0  | 1.0  |
| modaddCv       | -1.0  | 1.0  |

```{r include=FALSE}
## Load older versions of BayesianTools R routines needed in TG15
source("TG15-BayesianToolsOld.R")
source("helperFunctions.R")
```


# Identifying the issue

In this section we investigate the underlying issue that can cause problems when we try to calibrate a model with a data set that has very unbalanced numbers of observations from different parts of the system. We do this by breaking the problem into parts to investigate the individual influence of model structural error and data bias when calibrating with balanced and unbalanced datasets. We start with the idealised situation of a perfect and a perfect calibration dataset with a balanced number of observations for each part of the system. 

## Perfect model and balanced data Pb

Looking first at the parameters we find that the 'true' parameters are largely recaptured by the calibration. The marginal posterior distributions are centred around the 'truth' line and the uncertainty versus the prior has reduced significantly. The model outputs for NEE, Cv and Cs are also centred around the truth line with the 50% quantile line matching the truth line closely. The posterior uncertainty is small and the predictive interval matches the uncertainty in the data as would be expected. This first calibration can be considered as a control against which all subsequent calibrations can be compared.

```{r include=FALSE}
newPars <- refPars$best
names(newPars) = row.names(refPars)
parSel = c(defParms, nvar)
rm(obsSel)
isLow = NULL
likelihood <- function(x, sum = TRUE){
  x         <- createMixWithDefaultsOld(x, newPars, parSel)
  predicted <- VSEM(x[-nvar], PAR)
  diff       <- c(predicted[,1] - obs[,1])
  llValues1  <- dnorm(diff, sd = pmax((abs(c(predicted[,1])) + 0.0000001) * x[nvar],0.0005), log = T)
  diff       <- c(predicted[,2] - obs[,2])
  llValues2  <- dnorm(diff, sd = (abs(c(predicted[,2])) + 0.0000001) * x[nvar], log = T)
  diff       <- c(predicted[,3] - obs[,3])
  llValues3  <- dnorm(diff, sd = (abs(c(predicted[,3])) + 0.0000001) * x[nvar], log = T)

  ## if (sum == FALSE) return(llValues)
  ## else return(sum(llValues1,llValues2,llValues3))
  return(sum(llValues1,llValues2,llValues3))

}

prior         <- createUniformPrior(lower = refPars$lower[parSel], upper = refPars$upper[parSel])
out1 <- fitVSEM("run1.RData")

MAP  <- calcMAP("MAP.Rdata")
```


```{r echo=FALSE, out.width="45%", out.height="45%",fig.show='hold',fig.align='center', fig.cap="\\label{fig:Par}Parameters plot."}
knitr::include_graphics(c("/home/david/TG15/pDistrNew/KEXT.png","/home/david/TG15/pDistrNew/LUE.png","/home/david/TG15/pDistrNew/tauV.png","/home/david/TG15/pDistrNew/tauS.png","/home/david/TG15/pDistrNew/Cs.png","/home/david/TG15/pDistrNew/Cv.png"))
```

```{r echo=FALSE, background='white', fig.height=7, fig.cap="\\label{fig:perModbalDataOut}Perfect model, balanced data (NEE, Cv, Cs: 2048 obs). Observations included in the calibration marked with a '+'. Red line 50% quantile posterior distribution. Green line is the 'true' model output. Dark brown shading 2.5% 97.5% quantile posterior distribution. Light brown shading 2.5% 97.5% predictive interval."}
firstFig(out1,refPars)
```

```{r echo=FALSE, out.width="100%", out.height="100%",fig.show='hold',fig.align='center', fig.cap="\\label{fig:Par}Timeseries plot."}
knitr::include_graphics("timeseriesSelNew.png")
```


## Perfect model and unbalanced data Pu

We now consider what happens when we have a large imbalance in the calibration data. We do this by thinning out the number of observations for Cv from 2048 to just six observations whilst retaining the original 2048 observations for NEE and Cs; thus creating an O(3) imbalance. After calibration the parameters are still largely centred on the 'truth' line. For KEXT and especially tauV and Cv there has been an increase in marginal uncertainty but this would be expected since we have included less information in the calibration. In figure ... we see the outputs for Cv and Cs for Pu. For the remaining calibrations we do not include further plots of NEE as the plot does not show much change from that shown previously for Pb. For Cs also, there is little change from before (Pb) when the data was balanced. The Cv plot shows the six observations that were retained in the calibration. The posterior is still centred on the truth line with a larger posterior uncertainty as might be expected since far fewer data have been included. These results show that creating an imbalanced does not cause an issue in the calibration other than increasing the uncertainty. 

```{r include=FALSE}
newPars <- refPars$best
parSel = c(defParms, nvar)
obsSel <- c(1,202,390,550,750,920)*2.0
isLow = 2
likelihood <- function(x, sum = TRUE){
  x         <- createMixWithDefaults(x, newPars, parSel)
  predicted <- VSEM(x[-nvar], PAR)

  diff       <- c(predicted[,1] - obs[,1])
  llValues1  <- dnorm(diff, sd = pmax((abs(c(predicted[,1])) + 0.0000001) * x[nvar],0.0005), log = T)
  diff       <- c(predicted[obsSel,2] - obs[obsSel,2])
  llValues2  <- dnorm(diff, sd = (abs(c(predicted[obsSel,2])) + 0.0000001) * x[nvar], log = T)
  diff       <- c(predicted[,3] - obs[,3])
  llValues3  <- dnorm(diff, sd = (abs(c(predicted[,3])) + 0.0000001) * x[nvar], log = T)

  ## if (sum == FALSE) return(llValues)
  ## else return(sum(llValues1,llValues2,llValues3))
  return(sum(llValues1,llValues2,llValues3))
}

## Prior
prior         <- createUniformPrior(lower = refPars$lower[parSel], upper = refPars$upper[parSel])

out2 <- fitVSEM("run2.RData")

MAPunbal <- calcMAP("MAPunbal.Rdata")
```

## Model with error and balanced data Eb

Here we create a known significant structural error in the model by effectively removing the root pool from the model see section... After calibration a number of parameters are now quite far away from their 'true' values. This is especially dramatic for tauV, which controls the turnover of vegetation, and is now lower, so that the rate of turnover of the vegetation pool has now more than doubled. The error causes all the new carbon to be allocated to the vegetation pool only. The increased turnover rate tries to compensate for this error in the model. Hence, the large departure of the parameters from their 'true' values has the effect of 'absorbing' some of the influence of the model structural error. The result is that the model outputs have not changed significantly (see supplementary material) from the perfect model run. These results illustrate that model performance against calibration data can still be acceptable even when very signifcant model errors are present so long as changed parameters settings somewhat 'absorb' the influence of the error. 

```{r include=FALSE}

## Likelihood: Gaussian 2048 obs for each of NEE, Cv and Cs
newPars <- refPars$best
names(newPars) = row.names(refPars)
newPars["Av"]  <- 1.0
newPars["Cr"] <- 0.0
parSel = c(defParms, nvar)
rm(obsSel)
isLow = NULL
likelihood <- function(x, sum = TRUE){
  x         <- createMixWithDefaults(x, newPars, parSel)
  predicted <- VSEM(x[-nvar], PAR)
  diff       <- c(predicted[,1] - obs[,1])
  llValues1  <- dnorm(diff, sd = pmax((abs(c(predicted[,1])) + 0.0000001) * x[nvar],0.0005), log = T)
  diff       <- c(predicted[,2] - obs[,2])
  llValues2  <- dnorm(diff, sd = (abs(c(predicted[,2])) + 0.0000001) * x[nvar], log = T)
  diff       <- c(predicted[,3] - obs[,3])
  llValues3  <- dnorm(diff, sd = (abs(c(predicted[,3])) + 0.0000001) * x[nvar], log = T)

  ## if (sum == FALSE) return(llValues)
  ## else return(sum(llValues1,llValues2,llValues3))
  return(sum(llValues1,llValues2,llValues3))

}

prior         <- createUniformPrior(lower = refPars$lower[parSel], upper = refPars$upper[parSel])

out3 <- fitVSEM("run3.RData")

MAPErr <- calcMAP("MAPErr.Rdata")
```

## Model with error and unbalanced data Eu

We now combine the influences of unbalanced data and model error, investigated in the previous two sections, into the calibration. Looking first at the marginal parameter distributions after calibration there are changes versus the Eb calibration which are significant but not huge. In production, KEXT has increased and LUE has decreased slightly compensating for each other. Belowground, parameters Cs and tauS are now closer to their 'true' value than in Eb. Similarly aboveground, tauV is now closer to its 'true' value then in the Eb calibration. In general, the change in parameters to compensate for the model structural error is less than for Eb. Looking at outputs for Cv and Cs, the calibration is fine for Cs (and also NEE not shown) but drifts away significantly from the six vegetation measurements. This is the typical behaviour for calibrations with a large data imbalance, the sparsely measured parts of the system are ignored at the expense of the parts of the system with many observations. This calibration, along with the result from the previous two (Pu and Eb), make it clear that the model structural error is key in creating an issue when calibrating a model with a large imbalance in data. 

```{r include=FALSE}
newPars <- refPars$best
names(newPars) = row.names(refPars)
newPars["Av"]  <- 1.0
newPars["Cr"] <- 0.0
parSel = c(defParms, nvar)
obsSel <- c(1,202,390,550,750,920)*2.0
isLow = 2
likelihood <- function(x, sum = TRUE){
  x         <- createMixWithDefaults(x, newPars, parSel)
  predicted <- VSEM(x[-nvar], PAR)

  diff       <- c(predicted[,1] - obs[,1])
  llValues1  <- dnorm(diff, sd = pmax((abs(c(predicted[,1])) + 0.0000001) * x[nvar],0.0005), log = T)
  diff       <- c(predicted[obsSel,2] - obs[obsSel,2])
  llValues2  <- dnorm(diff, sd = (abs(c(predicted[obsSel,2])) + 0.0000001) * x[nvar], log = T)
  diff       <- c(predicted[,3] - obs[,3])
  llValues3  <- dnorm(diff, sd = (abs(c(predicted[,3])) + 0.0000001) * x[nvar], log = T)

  ## if (sum == FALSE) return(llValues)
  ## else return(sum(llValues1,llValues2,llValues3))
  return(sum(llValues1,llValues2,llValues3))
}

prior         <- createUniformPrior(lower = refPars$lower[parSel], upper = refPars$upper[parSel])

out5 <- fitVSEM("run5.RData")

MAPErrunbal <- calcMAP("MAPErrunbal.Rdata")
```

## Perfect model and balanced data with a multiplicative bias PbB

We now investigate the influence of data bias on the calibration. As presented in section..., we create a multiplicative data bias by multiplying the soil carbon pool by 0.8. Similarly to Eb, parameters in the calibration do not all recover their 'true' values and hence 'absorb' the influence of data error. As might be expected this is most dramatic for the belowground parameters. The initial Cs parameter decreases significantly and tauS also decreases, increasing the turnover. This has the effect of decreasing the soil carbon pool to match the erroneous data. As before, these departures of the parameters from their 'true' value allows there to be a reasonably close match between the model outputs after calibration and the data (supplementary material). 

```{r include=FALSE}

obs.orig     <- obs
obs[,3] <- obs[,3] * 0.8

newPars <- refPars$best
names(newPars) = row.names(refPars)
parSel = c(defParms, nvar)
rm(obsSel)
isLow = NULL
likelihood <- function(x, sum = TRUE){
  x         <- createMixWithDefaultsOld(x, newPars, parSel)
  predicted <- VSEM(x[-nvar], PAR)
  diff       <- c(predicted[,1] - obs[,1])
  llValues1  <- dnorm(diff, sd = pmax((abs(c(predicted[,1])) + 0.0000001) * x[nvar],0.0005), log = T)
  diff       <- c(predicted[,2] - obs[,2])
  llValues2  <- dnorm(diff, sd = (abs(c(predicted[,2])) + 0.0000001) * x[nvar], log = T)
  diff       <- c(predicted[,3] - obs[,3])
  llValues3  <- dnorm(diff, sd = (abs(c(predicted[,3])) + 0.0000001) * x[nvar], log = T)

  ## if (sum == FALSE) return(llValues)
  ## else return(sum(llValues1,llValues2,llValues3))
  return(sum(llValues1,llValues2,llValues3))
}

prior         <- createUniformPrior(lower = refPars$lower[parSel], upper = refPars$upper[parSel])

out12 <- fitVSEM("run12.RData")

obs <- obs.orig

#pdf('par12.pdf')
#plot(out12)
#dev.off()

#pdf('out12.pdf')
#plotOutputs(out12,refPars)
#dev.off()

```

## Perfect model and unbalanced data with a multiplicative bias PuB

We now add the effect of unbalanced data to the calibration with the significant data bias. We look first at the parameter marginal distributions after calibration. In carbon production, KEXT is now larger than its true value increasing the carbon inputted to the system. This is counteracted by a lower LUE. Aboveground Cv is smaller and tauV significantly larger decreasing the turnover to the soil. This has the combined effect of passing on less carbon to the soil. Belowground, tauS is slightly closer to its true value than PbB, Cs has increased versus the PbB calibration pushing it back towards its true value. As the output plots, show the calibration is now somewhat ignoring the six vegetation observations in a similar way to what was found for the calibration with a model error. The main the 'effort' in the calibration is on matching the many erroneous soil carbon observations. These results show there can be issues calibrating with unbalanced datasets whether there is a model structural error or a significant data bias. 


```{r include=FALSE}
obs[,3] <- obs[,3] * 0.8

newPars <- refPars$best
parSel = c(defParms, nvar)
obsSel <- c(1,202,390,550,750,920)*2.0
isLow = 2
likelihood <- function(x, sum = TRUE){
  x         <- createMixWithDefaultsOld(x, newPars, parSel)
  predicted <- VSEM(x[-nvar], PAR)

  diff       <- c(predicted[,1] - obs[,1])
  llValues1  <- dnorm(diff, sd = pmax((abs(c(predicted[,1])) + 0.0000001) * x[nvar],0.0005), log = T)
  diff       <- c(predicted[obsSel,2] - obs[obsSel,2])
  llValues2  <- dnorm(diff, sd = (abs(c(predicted[obsSel,2])) + 0.0000001) * x[nvar], log = T)
  diff       <- c(predicted[,3] - obs[,3])
  llValues3  <- dnorm(diff, sd = (abs(c(predicted[,3])) + 0.0000001) * x[nvar], log = T)

  ## if (sum == FALSE) return(llValues)
  ## else return(sum(llValues1,llValues2,llValues3))
  return(sum(llValues1,llValues2,llValues3))
}

prior         <- createUniformPrior(lower = refPars$lower[parSel], upper = refPars$upper[parSel])

out13 <- fitVSEM("run13.RData")

obs <- obs.orig

#pdf('par13.pdf')
#plot(out13)
#dev.off()

#pdf('out13.pdf')
#plotOutputs(out13,refPars)
#dev.off()

```

## Model with error and unbalanced data with a multiplicative bias EuB

Now we combine the model structural error with the data bias and run the calibration with the unbalanced dataset. The two errors reinforce each other since the erroneous increase in the vegetation pool due to the missing root pool model error add to the issue of trying to match the erroneously low soil carbon observations. THe additive effect of the two errors just increase the issues that are found with unbalanced data with the model outputs after calibration even further away from the six Cv observations. 

```{r include=FALSE}
#obs[,3] <- obs[,3] * 2.0
obs[,3] <- obs[,3] * 0.8

newPars <- refPars$best
names(newPars) = row.names(refPars)
newPars["Av"]  <- 1.0
newPars["Cr"] <- 0.0
parSel = c(defParms, nvar)
obsSel <- c(1,202,390,550,750,920)*2.0
isLow = 2
likelihood <- function(x, sum = TRUE){
  x         <- createMixWithDefaultsOld(x, newPars, parSel)
  predicted <- VSEM(x[-nvar], PAR)

  diff       <- c(predicted[,1] - obs[,1])
  llValues1  <- dnorm(diff, sd = pmax((abs(c(predicted[,1])) + 0.0000001) * x[nvar],0.0005), log = T)
  diff       <- c(predicted[obsSel,2] - obs[obsSel,2])
  llValues2  <- dnorm(diff, sd = (abs(c(predicted[obsSel,2])) + 0.0000001) * x[nvar], log = T)
  diff       <- c(predicted[,3] - obs[,3])
  llValues3  <- dnorm(diff, sd = (abs(c(predicted[,3])) + 0.0000001) * x[nvar], log = T)

  ## if (sum == FALSE) return(llValues)
  ## else return(sum(llValues1,llValues2,llValues3))
  return(sum(llValues1,llValues2,llValues3))
}

prior         <- createUniformPrior(lower = refPars$lower[parSel], upper = refPars$upper[parSel])

out17 <- fitVSEM("run17.RData")

obs <- obs.orig

#pdf('par17.pdf')
#plot(out17)
#dev.off()

#pdf('out17.pdf')
#plotOutputs(out17,refPars)
#dev.off()

```


# Diagnosing the issue

## Comparing model output with virtual data as truth.

Moving on from identifying the issue in the previous section, here we develop a tool for helping to diagnose at what point and to what extent having unbalanced data in Bayesian calibration (BC) becomes an issue when models and data are imperfect.

This is done by running a number of calibrations with perfect and imperfect models where the quantity and imbalance of data used increases with each calibration. Here we chose an increasing power series of two (2^3^,2^4^ ... 2^11^) for the increase in the quantity of calibration data; eight calibrations in all. In the balanced data BC case, quantities of NEE, vegetative carbon and soil carbon data included in the BC all increased in tandem in each subsequent calibration. For the unbalanced BC case, NEE and soil carbon data increased as before but the quantity of vegetative carbon data included in the BC was held fixed at six data points for each of the eight calibrations. After running the calibrations the VSEM was rerun with the maximum a posteriori (MAP) vector and the RMS difference with the 'true' data was calculated and plotted (Fig. \ref{fig:rmsMAPTruth}).

The figure shows broad similarity in results except for vegetative carbon when the model has an error and where there is an imbalanced in calibration data. In general, the RMS difference has a tendency to decrease as the quantity of data included in calibration increases. There is also a marked grouping of results with the perfect model getting closer to the data than the model with the error, as might be expected. For NEE and soil carbon with an imperfect model, the unbalanced calibration gets closer to the data than the balanced calibration especially as the quantity of calibration data increases. This is in marked contrast to vegetative carbon where RMS differences increase significantly as quantity of calibration data increases when the model has an error and when there is an imbalanced in calibration data. This increase in RMS difference for vegetative carbon occurs in tandem with the decreases noted already from NEE and soil carbon. This signature of increasing RMS difference for the low quantity data output versus the decreasing RMS difference for the high quantity can be used to diagnose when large imbalances in calibrations data with imperfect models and data start to become an issue. In this case, it appears after the quantity of data included in the calibration exceeds 32 but this will be different for each model, likelihood function and for each dataset used in calibrations.


```{r include=FALSE}
source("gridArrangeSharedLegend.R")

## csel <- map(2^{0:8},function(i) seq(1,2048,i))

## loadMAP <- function(fname){
##   load(paste(exptPath,fname,sep=""))
##   invisible(MAP)
## }

## MAP         <- loadMAP("MAP.Rdata")
## MAPunbal    <- loadMAP("MAPunbal.Rdata")
## MAPErr      <- loadMAP("MAPErr.Rdata")
## MAPErrunbal <- loadMAP("MAPErrunbal.Rdata")

calcRMS <- function(istep,fldno,tselect,fld,MAPfld){
    x          <- createMixWithDefaults(MAPfld[istep,], newPars, parSel)
    predicted  <- VSEM(x[-nvar], PAR)
    ofld       <- fld
    return(sqrt(mean((predicted[tselect,fldno] - ofld[tselect,fldno])**2)))
}

tt <- tibble::tibble ( istep=1:9)

newPars <- refPars$best
names(newPars) = row.names(refPars)
parSel = c(defParms, nvar)
newPars["Av"]  <- 1.0
newPars["Cr"]  <- 0.0
BCMAPRMSObs <- tt %>% mutate(NEERMSErrunbal=map_dbl(istep,function(i) calcRMS(i,fldno=1,tselect=1:ndays,fld=referenceData,MAPfld=MAPErrunbal))) %>%
                      mutate(CvRMSErrunbal =map_dbl(istep,function(i) calcRMS(i,fldno=2,tselect=1:ndays,fld=referenceData,MAPfld=MAPErrunbal))) %>%
                      mutate(CsRMSErrunbal =map_dbl(istep,function(i) calcRMS(i,fldno=3,tselect=1:ndays,fld=referenceData,MAPfld=MAPErrunbal))) %>%
                      mutate(NEERMSErr     =map_dbl(istep,function(i) calcRMS(i,fldno=1,tselect=1:ndays,fld=referenceData,MAPfld=MAPErr     ))) %>%
                      mutate(CvRMSErr      =map_dbl(istep,function(i) calcRMS(i,fldno=2,tselect=1:ndays,fld=referenceData,MAPfld=MAPErr     ))) %>%
                      mutate(CsRMSErr      =map_dbl(istep,function(i) calcRMS(i,fldno=3,tselect=1:ndays,fld=referenceData,MAPfld=MAPErr     ))) %>%
                      mutate(noObs=map_int(istep,function(i) length(csel[[i]])))

newPars["Av"]  <- 0.5
newPars["Cr"]  <- 3.0
BCMAPRMSObs <- BCMAPRMSObs %>%
                      mutate(NEERMSunbal   =map_dbl(istep,function(i) calcRMS(i,fldno=1,tselect=1:ndays,fld=referenceData,MAPfld=MAPunbal   ))) %>%
                      mutate(CvRMSunbal    =map_dbl(istep,function(i) calcRMS(i,fldno=2,tselect=1:ndays,fld=referenceData,MAPfld=MAPunbal   ))) %>%
                      mutate(CsRMSunbal    =map_dbl(istep,function(i) calcRMS(i,fldno=3,tselect=1:ndays,fld=referenceData,MAPfld=MAPunbal   ))) %>%
                      mutate(NEERMS        =map_dbl(istep,function(i) calcRMS(i,fldno=1,tselect=1:ndays,fld=referenceData,MAPfld=MAP        ))) %>%
                      mutate(CvRMS         =map_dbl(istep,function(i) calcRMS(i,fldno=2,tselect=1:ndays,fld=referenceData,MAPfld=MAP        ))) %>%
                      mutate(CsRMS         =map_dbl(istep,function(i) calcRMS(i,fldno=3,tselect=1:ndays,fld=referenceData,MAPfld=MAP        )))

p1 <- ggplot(data = BCMAPRMSObs, aes(x=noObs)) +
    geom_point(aes(y = NEERMS,colour="Perfect Model")) +
    geom_line (aes(y = NEERMS,colour="Perfect Model")) +
    geom_point(aes(y = NEERMSunbal,colour="Perfect Model UnBal Data")) +
    geom_line (aes(y = NEERMSunbal,colour="Perfect Model UnBal Data")) +
    geom_point(aes(y = NEERMSErr,colour="Model with Error")) +
    geom_line (aes(y = NEERMSErr,colour="Model with Error")) +
    geom_point(aes(y = NEERMSErrunbal,colour="Model with Error UnBal Data")) +
    geom_line (aes(y = NEERMSErrunbal,colour="Model with Error UnBal Data"))+
    labs(x = " ", y="RMS NEE")

p2 <- ggplot(data = BCMAPRMSObs, aes(x=noObs)) +
    geom_point(aes(y = CvRMS,colour="Perfect Model")) +
    geom_line (aes(y = CvRMS,colour="Perfect Model")) +
    geom_point(aes(y = CvRMSunbal,colour="Perfect Model UnBal Data")) +
    geom_line (aes(y = CvRMSunbal,colour="Perfect Model UnBal Data")) +
    geom_point(aes(y = CvRMSErr,colour="Model with Error")) +
    geom_line (aes(y = CvRMSErr,colour="Model with Error")) +
    geom_point(aes(y = CvRMSErrunbal,colour="Model with Error UnBal Data"))+
    geom_line (aes(y = CvRMSErrunbal,colour="Model with Error UnBal Data"))+
    labs(x = " ", y="RMS vegetative carbon")

p3 <- ggplot(data = BCMAPRMSObs, aes(x=noObs)) +
    geom_point(aes(y = CsRMS,colour="Perfect Model")) +
    geom_line (aes(y = CsRMS,colour="Perfect Model")) +
    geom_point(aes(y = CsRMSunbal,colour="Perfect Model UnBal Data")) +
    geom_line (aes(y = CsRMSunbal,colour="Perfect Model UnBal Data")) +
    geom_point(aes(y = CsRMSErr,colour="Model with Error")) +
    geom_line (aes(y = CsRMSErr,colour="Model with Error")) +
    geom_point(aes(y = CsRMSErrunbal,colour="Model with Error UnBal Data")) +
    geom_line (aes(y = CsRMSErrunbal,colour="Model with Error UnBal Data"))+
    labs(x = "Number of observations included in the calibration", y="RMS soil carbon")
```

```{r echo=FALSE, background='white', fig.height=7, fig.cap="\\label{fig:rmsMAPTruth}Each point in the three graphs (NEE, vegetative carbon, and soil carbon) represents the RMS difference between the VSEM model and the 'truth' run with different maximum a posteriori (MAP) vectors. The MAP vector at each point is obtained from a Bayesian calibration (BC) where the quantity of data included in the BC increases in a sequence along the x-axis following the exponentiation of base two. For the balanced calibration case (red and cyan) vegetative carbon data increases in tandem with NEE and soil carbon. For the unbalanced calibration case (green and purple) the quantity of vegetative carbon data is held fixed at six data values for each point along the x-axis. The VSEM model is either 'perfect' (cyan and purple) or has a known error (red and green) relative to the 'true' data that was derived from it."}
grid_arrange_shared_legend(p1, p2, p3, ncol = 1, nrow = 3)

```

## Comparing model output against "obervations"

The diagnosis made in the previous section had the benefit of access to the 'true' data and a perfect model. Unfortunately this is never the case for real world ecological model calibrations. Therefore, here we have repeated the previous graph Fig.(\ref{fig:rmsMAPTruth}) with just the imperfect model and the imbalanced calibration, but with RMS differences now calculated against observations (NEE: 2048 points, vegetative carbon: 6 points, soil carbon: 2048 points) (Fig. \ref{fig:rmsMAPObs}). While there are clear differences in the RMS values versus the previous graph, as might be expected, the broad-scale signature of increasing RMS difference for vegetative carbon and decreasing RMS difference for NEE and soil carbon is retained. As before, this graph can be used to diagnose when the imbalanced in data is starting to interact with the erroneous model. In this case, as before, this occurs for a data quantity greater than 32.

```{r include=FALSE}
tt <- tibble::tibble ( istep=1:9)

obsSel <- c(1,202,390,550,750,920)*2.0

newPars["Av"]  <- 1.0
newPars["Cr"]  <- 0.0
BCMAPRMSObs <- tt %>% mutate(NEERMSErrunbal=map_dbl(istep,function(i) calcRMS(i,fldno=1,tselect=1:ndays,fld=obs,MAPfld=MAPErrunbal))) %>%
                      mutate(CvRMSErrunbal =map_dbl(istep,function(i) calcRMS(i,fldno=2,tselect=obsSel,fld=obs,MAPfld=MAPErrunbal))) %>%
                      mutate(CsRMSErrunbal =map_dbl(istep,function(i) calcRMS(i,fldno=3,tselect=1:ndays,fld=obs,MAPfld=MAPErrunbal))) %>%
                      mutate(noObs=map_int(istep,function(i) length(csel[[i]])))

p1 <- ggplot(data = BCMAPRMSObs, aes(x=noObs)) +
    geom_point(aes(y = NEERMSErrunbal,colour="Model with Error UnBal Data")) +
    geom_line (aes(y = NEERMSErrunbal,colour="Model with Error UnBal Data")) +
    labs(x = " ", y="RMS NEE")

p2 <- ggplot(data = BCMAPRMSObs, aes(x=noObs)) +
    geom_point(aes(y = CvRMSErrunbal,colour="Model with Error UnBal Data"))+
    geom_line (aes(y = CvRMSErrunbal,colour="Model with Error UnBal Data"))+ 
    labs(x = " ", y="RMS vegetative carbon")

p3 <- ggplot(data = BCMAPRMSObs, aes(x=noObs)) +
    geom_point(aes(y = CsRMSErrunbal,colour="Model with Error UnBal Data")) +
    geom_line (aes(y = CsRMSErrunbal,colour="Model with Error UnBal Data")) +
    labs(x = "Number of observations included in the calibration", y="RMS soil carbon")
```

```{r echo=FALSE, background='white', fig.height=7, fig.cap="\\label{fig:rmsMAPObs}Each point in the three graphs (NEE, vegetative carbon, and soil carbon) represents the RMS difference between the VSEM model and virtual observations run with different maximum a posteriori (MAP) vectors. The MAP vector at each point is obtained from a Bayesian calibration (BC) where the quantity of data included in the BC for NEE and soil carbon increases in a sequence along the x-axis following the exponentiation of base two. The quantity of vegetaive carbon data is held fixed at six for all points in the graphs. The VSEM model used has a known error relative to the virtual observations that was derived from it."}
grid_arrange_shared_legend(p1, p2, p3, ncol = 1, nrow = 3)
```

# Changes to the Likelihood to represent model and data errors

The results from section... demonstrate that the underlying issue with including unbalanced data in the calibration is not the imbalance itself but that there are significant model structural errors or data systematic biases or both effecting the calibration. Therefore, here we aim to introduce terms in the likelihood which represent our uncertainty about what these errors could be. This uncertainty exists and hence it needs to be represented. Otherwise, as we saw in section..., the posterior uncertainty will be too small and parameters will find their greatest posterior probability far from their 'true' values, to try and compensate for the unrepresented errors in the calibration. 
	
As presented in section... here we represent these model and data errors as simply as possible using additive and multiplicative terms in the calibration data for NEE, Cv and Cs. This introduces six new parameters to the calibration. 

## Model with error and unbalanced perfect data with additive and multiplicative parameters to represent model error. EuL

Our first test using the new Likelihood is with the calibration with a significant model error and unbalanced data. To see the influence of the new terms we compare marginal posterior parameter distribution with those for Eu. An important effect of introducing these new terms is that uncertainty is in general increased. For some parameters (LUE, Cs and error-coeff) the posterior distribution is closer to the 'true' value. Others (tauV and tauS) are centred further away from their 'true' value but with larger uncertainty. Looking at the output timeseries the influence of the error hasn't gone away, as already noted for posterior parameter distributions, but there has been a significant improvement with the centre of the posterior now much closter to the 'truth' line. In addition, the uncertainty has increased so that 5 of the 6 data points are now inside the posterior confidence interval. The very simple multiplicative and additive terms introduced have not removed the influence of the error suggesting that more complex terms may be beneficial. There is however, a much greater sense that the sparse data are influencing the calibration. 

 
```{r include=FALSE}
addPars                   <- refPars
addPars[nvar+1,]          <- c(1.0, 0.1, 2.0)
addPars[nvar+2,]          <- c(1.0, 0.1, 2.0)
addPars[nvar+3,]          <- c(1.0, 0.1, 2.0)
addPars[nvar+4,]          <- c(0.0, -0.01, 0.01)
addPars[nvar+5,]          <- c(0.0, -1.0, 1.0)
addPars[nvar+6,]          <- c(0.0, -1.0, 1.0)
rownames(addPars)[nvar+1]   <- "modmultNEE"
rownames(addPars)[nvar+2]   <- "modmultCs"
rownames(addPars)[nvar+3]   <- "modmultCv"
rownames(addPars)[nvar+4]   <- "modaddNEE"
rownames(addPars)[nvar+5]   <- "modaddCs"
rownames(addPars)[nvar+6]   <- "modaddCv"
newPars <- addPars$best
names(newPars) = row.names(addPars)
newPars["Av"]  <- 1.0
newPars["Cr"]  <- 0.0
parSel = c(defParms, nvar,nvar+1,nvar+2,nvar+3,nvar+4,nvar+5,nvar+6)
npar <- length(parSel)
obsSel <- c(1,202,390,550,750,920)*2
isLow = 2
likelihood <- function(x, sum = TRUE){
  x         <- createMixWithDefaults(x, newPars, parSel)
  predicted <- VSEM(x[1:(nvar-1)], PAR)

  diff       <- c((predicted[,1]*x[nvar+1] + x[nvar+4]) - obs[,1])
  llValues1  <- dnorm(diff, sd = pmax((abs(c(predicted[,1])) + 0.0000001) * x[nvar],0.0005), log = T)
  diff       <- c(((predicted[1,2] + (predicted[obsSel,2] - predicted[1,2])*x[nvar+3]) + x[nvar+6]) - obs[obsSel,2])
  llValues2  <- dnorm(diff, sd = (abs(c(predicted[obsSel,2])) + 0.0000001) * x[nvar], log = T)
  diff       <- c(((predicted[1,3] + (predicted[,3] - predicted[1,3])*x[nvar+2]) + x[nvar+5]) - obs[,3])
  llValues3  <- dnorm(diff, sd = (abs(c(predicted[,3])) + 0.0000001) * x[nvar], log = T)

  ## if (sum == FALSE) return(llValues)
  ## else return(sum(llValues1,llValues2,llValues3))
  return(sum(llValues1,llValues2,llValues3))
}

prior         <- createUniformPrior(lower = addPars$lower[parSel], upper = addPars$upper[parSel])

out15 <- fitVSEM("run15.RData",iter=1200000, params=addPars)

#pdf('par15.pdf')
#plot(out15)
#dev.off()

#pdf('out15.pdf')
#plotOutputsEsys2(out15,addPars)
#dev.off()

```

## Perfect model and and unbalanced data with a multiplicative bias and additive and multiplicative parameters to represent the bias. PuBL

This calibration introduces the Likelikehood error terms to the previous calibration (PuB) with a large data bias and unbalanced data. Looking at changes in the posterior parameter distributions versus PuB, as noted previously for EuL, a key influence is that uncertainty is increased. Some parameters (KEXT, tauV, Cv) are significantly closer to their true values. As for EuL the vegetative carbon timeseries is also much improved (supplementary material). This shows that the extra terms are having a similar beneficial influence on this calibration as was found in EuL. 

```{r include=FALSE}
obs[,3] <- obs[,3] * 0.8

addPars                   <- refPars
addPars[nvar+1,]          <- c(1.0, 0.1, 2.0)
addPars[nvar+2,]          <- c(1.0, 0.1, 2.0)
addPars[nvar+3,]          <- c(1.0, 0.1, 2.0)
addPars[nvar+4,]          <- c(0.0, -0.01, 0.01)
addPars[nvar+5,]          <- c(0.0, -1.0, 1.0)
addPars[nvar+6,]          <- c(0.0, -1.0, 1.0)
rownames(addPars)[nvar+1]   <- "modmultNEE"
rownames(addPars)[nvar+2]   <- "modmultCs"
rownames(addPars)[nvar+3]   <- "modmultCv"
rownames(addPars)[nvar+4]   <- "modaddNEE"
rownames(addPars)[nvar+5]   <- "modaddCs"
rownames(addPars)[nvar+6]   <- "modaddCv"
newPars <- addPars$best
names(newPars) = row.names(addPars)
parSel = c(defParms, nvar,nvar+1,nvar+2,nvar+3,nvar+4,nvar+5,nvar+6)
npar <- length(parSel)
obsSel <- c(1,202,390,550,750,920)*2
isLow = 2
likelihood <- function(x, sum = TRUE){
  x         <- createMixWithDefaults(x, newPars, parSel)
  predicted <- VSEM(x[1:(nvar-1)], PAR)

  diff       <- c((predicted[,1]*x[nvar+1] + x[nvar+4]) - obs[,1])
  llValues1  <- dnorm(diff, sd = pmax((abs(c(predicted[,1])) + 0.0000001) * x[nvar],0.0005), log = T)
  diff       <- c(((predicted[1,2] + (predicted[obsSel,2] - predicted[1,2])*x[nvar+3]) + x[nvar+6]) - obs[obsSel,2])
  llValues2  <- dnorm(diff, sd = (abs(c(predicted[obsSel,2])) + 0.0000001) * x[nvar], log = T)
  diff       <- c(((predicted[1,3] + (predicted[,3] - predicted[1,3])*x[nvar+2]) + x[nvar+5]) - obs[,3])
  llValues3  <- dnorm(diff, sd = (abs(c(predicted[,3])) + 0.0000001) * x[nvar], log = T)

  ## if (sum == FALSE) return(llValues)
  ## else return(sum(llValues1,llValues2,llValues3))
  return(sum(llValues1,llValues2,llValues3))
}

prior         <- createUniformPrior(lower = addPars$lower[parSel], upper = addPars$upper[parSel])

out16 <- fitVSEM("run16.RData",iter=1200000,params=addPars)

#pdf('par16.pdf')
#plot(out16)
#dev.off()

#pdf('out16.pdf')
#plotOutputsEsys2(out16,addPars)
#dev.off()

obs <- obs.orig

```

## Model with error and and unbalanced data with a multiplicative bias and additive and multiplicative parameters to represent model error and the data bias. EuBL

In this final calibration we combine the influence of the model error, the data bias and the unbalanced now with the new terms in the likelihood representing additive and multiplicative error. We compare against the posterior parameter distributions for the calibration EuB. Similarly to the previous calibrations (EuL and PuBL) the uncertainty has increased significantly for a number of parameters (LUE, tauS, tauV and Cs). A number of parameter distributions are now closer to parameter's 'true' value (KEXT, tauV and Cv). As might be expected the resultant posterior parameter distribution are somewhat a combination of what we see for EuL and PuBL. For the Cv pool there is a large improvement in the fit to data with five of the six data points now within the posterior prediction.  


```{r include=FALSE}
obs[,3] <- obs[,3] * 0.8

addPars                   <- refPars
addPars[nvar+1,]          <- c(1.0, 0.1, 2.0)
addPars[nvar+2,]          <- c(1.0, 0.1, 2.0)
addPars[nvar+3,]          <- c(1.0, 0.1, 2.0)
addPars[nvar+4,]          <- c(0.0, -0.01, 0.01)
addPars[nvar+5,]          <- c(0.0, -1.0, 1.0)
addPars[nvar+6,]          <- c(0.0, -1.0, 1.0)
rownames(addPars)[nvar+1]   <- "modmultNEE"
rownames(addPars)[nvar+2]   <- "modmultCs"
rownames(addPars)[nvar+3]   <- "modmultCv"
rownames(addPars)[nvar+4]   <- "modaddNEE"
rownames(addPars)[nvar+5]   <- "modaddCs"
rownames(addPars)[nvar+6]   <- "modaddCv"
newPars <- addPars$best
names(newPars) = row.names(addPars)
newPars["Av"]  <- 1.0
newPars["Cr"]  <- 0.0
parSel = c(defParms, nvar,nvar+1,nvar+2,nvar+3,nvar+4,nvar+5,nvar+6)
npar <- length(parSel)
obsSel <- c(1,202,390,550,750,920)*2
isLow = 2
likelihood <- function(x, sum = TRUE){
  x         <- createMixWithDefaults(x, newPars, parSel)
  predicted <- VSEM(x[1:(nvar-1)], PAR)

  diff       <- c((predicted[,1]*x[nvar+1] + x[nvar+4]) - obs[,1])
  llValues1  <- dnorm(diff, sd = pmax((abs(c(predicted[,1])) + 0.0000001) * x[nvar],0.0005), log = T)
  diff       <- c(((predicted[1,2] + (predicted[obsSel,2] - predicted[1,2])*x[nvar+3]) + x[nvar+6]) - obs[obsSel,2])
  llValues2  <- dnorm(diff, sd = (abs(c(predicted[obsSel,2])) + 0.0000001) * x[nvar], log = T)
  diff       <- c(((predicted[1,3] + (predicted[,3] - predicted[1,3])*x[nvar+2]) + x[nvar+5]) - obs[,3])
  llValues3  <- dnorm(diff, sd = (abs(c(predicted[,3])) + 0.0000001) * x[nvar], log = T)

  ## if (sum == FALSE) return(llValues)
  ## else return(sum(llValues1,llValues2,llValues3))
  return(sum(llValues1,llValues2,llValues3))
}

prior         <- createUniformPrior(lower = addPars$lower[parSel], upper = addPars$upper[parSel])

out18 <- fitVSEM("run18.RData",iter=1200000, params=addPars)

#pdf('par18.pdf')
#plot(out18)
#dev.off()

#pdf('out18.pdf')
#plotOutputsEsys2(out18,addPars)
#dev.off()


## addPars                   <- refPars
## addPars[nvar+1,]          <- c(1.0, 0.1, 3.0)
## addPars[nvar+2,]          <- c(1.0, 0.1, 3.0)
## addPars[nvar+3,]          <- c(0.0, -0.01, 0.01)
## addPars[nvar+4,]          <- c(0.0, -1.0, 1.0)
## rownames(addPars)[nvar+1]   <- "modmultNEE"
## rownames(addPars)[nvar+2]   <- "modmultCs"
## rownames(addPars)[nvar+3]   <- "modaddNEE"
## rownames(addPars)[nvar+4]   <- "modaddCs"
## newPars <- addPars$best
## names(newPars) = row.names(addPars)
## newPars["Av"]  <- 1.0
## newPars["Cr"]  <- 0.0
## parSel = c(defParms, nvar,nvar+1,nvar+2,nvar+3,nvar+4)
## npar <- length(parSel)
## obsSel <- c(1,202,390,550,750,920)*2
## isLow = 2
## likelihood <- function(x, sum = TRUE){
##   x         <- createMixWithDefaultsOld(x, newPars, parSel)
##   predicted <- VSEM(x[1:(nvar-1)], PAR)

##   diff       <- c((predicted[,1]*x[nvar+1] + x[nvar+3]) - obs[,1])
##   llValues1  <- dnorm(diff, sd = pmax((abs(c(predicted[,1])) + 0.0000001) * x[nvar],0.0005), log = T)
##   diff       <- c(predicted[obsSel,2] - obs[obsSel,2])
##   llValues2  <- dnorm(diff, sd = (abs(c(predicted[obsSel,2])) + 0.0000001) * x[nvar], log = T)
##   diff       <- c(((predicted[1,3] + (predicted[,3] - predicted[1,3])*x[nvar+2]) + x[nvar+4]) - obs[,3])
##   llValues3  <- dnorm(diff, sd = (abs(c(predicted[,3])) + 0.0000001) * x[nvar], log = T)

##   ## if (sum == FALSE) return(llValues)
##   ## else return(sum(llValues1,llValues2,llValues3))
##   return(sum(llValues1,llValues2,llValues3))
## }

## prior         <- createUniformPrior(lower = addPars$lower[parSel], upper = addPars$upper[parSel])

## out18 <- fitVSEM("run18.RData",iter=1200000,params=addPars)
```

# Discussion

## Unbalanced data in Bayesian calibration: Identifying the issue

Our aim initially was to illustrate as clearly as possible the root cause behind why including unbalanced data in Bayesian Calibration (BC) can cause challenges. Firstly we illustrated that there was no issue with including very unbalanced data in BC if the data had unbiased errors and the model was perfect. So there is no intrinsic issue in using very unbalanced data in BC, the issues that we often find must be due to something else. Next we introduced a very significant model error (removing an important model process) but we initially kept the quantities of data balanced. The model predictions after BC were close to the data, which is typically what we find when using balanced data in BC. In a 'real world' calibration, with data as measurements taken from the environment, we do not know the extent of the model error present nor the best or true settings of the parameters, so this BC with balanced data would be considered a successful calibration. In our artificial BC we have access to information that we would not typically have because we know the true parameter settings. Hence, we find that after BC the parameters are far from their true values with high confidence. 'From the perspective of the calibration' the goal is to diminish the model-data difference. The likelihood cannot distinguish between model-data difference due to parameter or model structural error and has no means to change the structure of the model so model-data difference is reduced by solely by the parameters departing significantly from their true values. In this way, the calibration 'absorbs' the model error into wrong settings of the parameters so that model predictions against the data used in the calibration are acceptable. Other model predictions may be very poor but we have no other data available to assess this. Therefore, in this 'balanced data' calibration the model error is giving problems in the BC but these issues would normally be hidden from us and we would deem the calibration a success. It is normally only when we include unbalanced data in the calibration that the model predictions against the calibration data are poor that we identify an issue in the calibration and potentially wrongly perceive that it is an issue with the unbalanced data. Indeed, while the model predictions are poorer after calibration with the unbalanced data, the parameters are if anything closer to their true value and less confidently wrong because the erroneous model is fitted to fewer data points and from the perspective of the calibration it is not so important that the model-data difference is larger for the output with fewer observations present, hence the parameters need not depart so much from their true values to reduce model-data difference for the majority of the data points included in the calibration. These idealised calibrations demonstrate cleanly that the underlying issue with calibrating models with unbalanced data is not the unbalance in the data, but that models have hidden structural deficiencies that often remain hidden when we calibrate with balanced datasets but whose influence is seen in poor predictions after calibration with unbalanced datasets. The underlying issue is that when the model is wrong that model-data differences in the likelihood is reduced by the parameters departing from their true or best value. This is noticed in the unbalanced calibration but hidden in the balanced calibration. The root issue identified here is the presence of the model error not any imbalance in the calibration data. 

Further, we repeated the above analysis but now included a large bias in the calibration data rather than an error in the model. We find very similar results because 'from the perspective of the calibration' what we have introduced is just a slightly different model-data difference that as before is reduced in the calibration by setting the parameter values away from their true values so that the data bias is effectively absorbed by erroneous parameter values. Here we have demonstrated that very similar issues occur with unbalanced data whether we have a model error or a data bias because in the likelihood and hence the calibration, the issue is effectively the same. The likelihood cannot distinguish whether the model-data difference is due to model systematic error, data bias or poorly set parameters and the only means to reduce the difference is to change the parameter settings. As for the previous case, with model error, the underlying data bias can remain hidden and it is possible to erroneously perceive that the issue in the calibration is due to unbalanced data rather than a systematic bias in the data. 

## Diagnostic tool (Fig. \ref{fig:rmsMAPObs})

In the virtual experiments that we have presented here it is relatively easy to perceive what is happening because we have access to the true parameter settings and also to the 'true' system timeseries. In a 'real-world' calibrations it can be much harder to identify why a calibration is going wrong. Our aim here was to first identify the characteristic behaviour or signature that model or data errors or both are contributing to issues calibrating with unbalanced datasets. We first use the privileged access that we have here to the truth and then go on to develop a tool that could be used in a real-world calibration. We first illustrated with the perfect model (Fig. \ref{fig:rmsMAPTruth}) that starting with a balanced calibration dataset that RMS output error goes down for all the model outputs when the quantity of data in the calibration increases, increasing the imbalance. In contradistinction when the model error is present (Fig. \ref{fig:rmsMAPObs}), the RMS output error increases as the quantity of data in the calibration increases (increasing the imbalance) for the sparse data output as the RMS output error decreases for then plentiful data outputs. This is the signature behaviour that demonstrates and hence diagnoses the influence of the model discrepancy (or data bias) on the calibration. Indeed, when we repeat these set of eight calibrations in a more real-world setting where the RMS is calculated with the observational data rather than the 'true' data we see the same signature. That is to say starting with a balanced calibration and adding more observations thus increasing the imbalance, the predictions of the plentifully observed outputs improve whilst the poorly observed outputs get worse. We think that this tool can be useful as a diagnostic because as well as giving a signature curve of the issue the curves show clearly when the imbalance in the data starts to have a significant influence with one curve increasing whilst the others decrease.

## Reweighting the calibration data to restore balance

We argued in the introduction that using ad-hoc methods such as thinning the calibration data to give a more balanced dataset was the wrong approach. The idealised experiments that we have conducted in this study provide another good reason to avoid ad-hoc data reliability changing methods. It is clear that the issue that lies behind the challenge of calibrating ecological PBMs with unbalanced datasets is not the imbalance itself but rather the systematic errors in the model and the data which are always present to some degree. So it is much better to 'treat' the underlying cause of the problem rather than try and mitigate the symptoms. Ideally of course, the very best approach would be to make changes to the model and the data collections to eradicate the damaging systematic and structural errors. In reality unfortunately, it is only possible to achieve this to some extent and we are left with the need to continue to make useful predictions with our imperfect models informed by imperfect data. Making predictions with imperfect models is no problem so long as we have a good estimation of the reliability that is to say that we can accurately quantify the uncertainty in the model predictions. In BC we infer the posterior uncertainty in the model parameters but as we illustrate here it is also very important to quantify the uncertainty due to model structural errors and data systematic biases. In making such an estimation we take into account the uncertainties in the model predictions and we also hope to avoid the overfitting of the model parameters to 'absorb' the errors in the model and data which leads to the erroneous calibrations that we have illustrated herein. 

## Illustrating the benefits of including terms int he calibration that represent model and data error

For the reason given above in the final section of this study we included very simple linear terms to try and represent our uncertainty about model structural errors and data systematic biases. The topic of identifying and creating good statistical models of model discrepancy (and data bias) is not straightforward and being actively researched in its own right and is outwith the remit of this study. Our purpose here is merely to illustrate the kinds of benefits that can be expected when such terms are introduced. The simple representations used here in these very idealised calibrations are not offered as realistic solutions that could be used with real model and data in BC. Nevertheless, as in all modelling we advocate beginning simple approaches to begin with as we have followed here and only complicating the modelling should that be necessary. As the results make clear the simple terms that we have used did make a significant improvement to the model predictions but were only partially successful in recovering the true parameter setting. To go further in a real world setting we would need to consider more complex terms. Since our purpose here is illustrative and to motivate a greater efforts to be made in model discrepancy modelling rather than to provide solutions which in any case will be very model and context specific.

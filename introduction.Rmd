# Introduction

Big Picture
* Era of rapid change (transients). Urgent need to better understand and predict natural systems
* Era of big data -- unprecendented capacity to do so
* Multiple constraints incredibly important to getting the right answer for the right reason (Medlyn)
* Large push to do more data fusion (= statistical assimilation of multiple constraints), particularly within process models

We live in an era of rapid environmental change, with multiple drivers of ecosystem processes shifting simultaneously (climate, CO2, nutrient deposition, pollution, species introductions, habitat destruction) and continuously. Because of this the natural systems that we seek to understand, manage, and conserve are in a period dominated by transient conditions and will continue to be so for the foreseeable future. In the face of this change there is an urgent need for ecologists and other environmental scientists to be able to better understand and predict both these transient dynamics and their long-term implications (Dietze et al. 2018). Fortunately, in many cases we are aided in this endeavor by an increasing volume, variety, and velocity of environmental data (CITE LaDeau, Jack Williams, RS, etc.) as well as increasingly sophisticated models (model review CITE?). 





 Main issue

* In principle data fusion is straightforward -- one writes down statistical data models (Likelihoods) for each data constraint that are each linked to the same underlying process model (Zipkin, etc.)
* In practice, there are some real 'gotchas' where things can go awry, and folks often respond to these with ad hoc 'bandaids'. Failure to investigate the underlying issue and address with solutions that have a degree of objectivity and don't violate statistical principles.

* One of the most common 'gotchas' occurs when one has unbalanced data...

  * Because of the difference in cost in collecting automatic
    measurements such as eddy covariance data and manual measurements
    such as soil and plant carbon stocks, there is often two orders of
    magnitude difference or more imbalance between the number of
    measurements available from different parts of the forest
    ecosystem.
  * Since each measurement point normally counts as an independent piece of
    data in the Likelihood of Bayesian calibration the influence of
    the sparse observations can often be overwhelmed by the higher
    frequency data (Cameron et al (in review 2018). 
  * As more and more EO data becomes available (e.g. Sentinal) this
    issue of extremely imbalanced datasets is likely to worsen
    significantly.
	
 What is the effective information content (IC) of observations
 (possibly include or is this really a different issue? - discuss
 Mike/Marcel)

  * One eddy covariance tower with 17000 measurements count this as n = 1 or n = 17000 obs?
  * Aggregate 5 min data to 10 min retain essentially same IC but sample size halved!
  * Spectral analysis of eddy covariance NEE data two peaks: annual(seasonal) and diurnal
  * Often assume in BC that each data*point provides independent information
  * If biases in data or model errors are not independent

 To-date solutions generally ad hoc and/or arbitrary 
  
  * Literature review...
  * Ignore but then models over-fitted and uncertainty underestimated
  * Apply arbitrary weights to rebalance influence of data in BC
  * Thin the number of eddy covariance obs
      * throwing away useful information

 Purpose of paper:
 
  1. To identify the issue of using unbalanced data in parameter calibration of ecosystem models using artificial experiments. 
  2. Develop a general methodology for identifying whether and where the issue becomes a problem.
  3. Start to explore the simplest modifications to the likelihood to represent model structural error and data bias to help ameliorate the issue identified in (1). 

 Artificial experiments:
 
  * Developed a very simple process-besed ecosystem model (VSEM) as a testbed for
    experiments. Simple enough that results are easily
    understood. Sufficiently complex that we can be confident that the
    model/data issues identified here would also been seen in more
    complex/realistic process-based ecosystem models. 
  * Calibration data model's own output to control:
	1. test influence of model and data perfection/imperfection
	2.  test influence of balance/unbalance of data in the calibration
